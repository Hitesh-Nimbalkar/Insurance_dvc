{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from Insurance.exception import InsuranceException\n",
    "from mlflow.tracking import MlflowClient\n",
    "from Insurance.utils import load_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiments_evaluation:\n",
    "    def __init__(self,experiment_name,run_name) :\n",
    "        self.experiment_name=experiment_name\n",
    "        self.run_name=run_name\n",
    "        \n",
    "        self.run_id=None\n",
    "        self.model_name=model_name\n",
    "    # Function to save run details to YAML file\n",
    "    def save_run_details_to_yaml(self,run_id,model_name,model_uri,run_name):\n",
    "        client = MlflowClient()\n",
    "        run = client.get_run(run_id=run_id)\n",
    "        \n",
    "        report = {\n",
    "            \"Experiment\": self.experiment_name,\n",
    "            \"Model_name\":model_name,\n",
    "            \"run_name\":run_name,\n",
    "            \"run_id\": run.info.run_id,\n",
    "            \"experiment_id\": run.info.experiment_id,\n",
    "            \"Parameters\": run.data.params,\n",
    "            \"metrics\": run.data.metrics,\n",
    "            \"Model_uri\": model_uri\n",
    "        }\n",
    "            \n",
    "        return report\n",
    "            \n",
    "            \n",
    "    def get_best_model_run_id(self,experiment_name, metric_name):\n",
    "        # Get the experiment ID\n",
    "        experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "        \n",
    "        # Retrieve runs and sort by the specified metric\n",
    "        runs = mlflow.search_runs(experiment_ids=[experiment_id], filter_string='', order_by=[f\"metrics.{metric_name} DESC\"])\n",
    "        \n",
    "        if runs.empty:\n",
    "            print(\"No runs found for the specified experiment and metric.\")\n",
    "            return None\n",
    "        \n",
    "        # Get the best run\n",
    "        best_run = runs.iloc[0]\n",
    "        best_run_id = best_run.run_id\n",
    "        \n",
    "        # Load the best model\n",
    "      #  best_model = mlflow.sklearn.load_model(f\"runs:/{best_run_id}/model\")\n",
    "        model_uri=(f\"runs:/{best_run_id}/model\")\n",
    "        return model_uri,best_run_id\n",
    "\n",
    "    def run_mlflow_experiment(self, R2_score, parameters, model_path):\n",
    "        \"\"\"\n",
    "        Run an MLflow experiment, log metrics, parameters, and a model,\n",
    "        and return the best model and its run ID based on the specified metric.\n",
    "        \n",
    "        Parameters:\n",
    "        - experiment_name (str): Name of the MLflow experiment.\n",
    "        - run_name (str): Name of the run.\n",
    "        - R2_score (float): R2 score to be logged as a metric.\n",
    "        - parameters (dict): Dictionary of parameters to be logged.\n",
    "        - model: The machine learning model to be logged.\n",
    "        \n",
    "        Returns:\n",
    "        - best_model (object): The best model from the experiment.\n",
    "        - best_run_id (str): ID of the best run based on the specified metric.\n",
    "        \"\"\"\n",
    "        # Create or get the experiment\n",
    "        mlflow.set_experiment(self.experiment_name)\n",
    "        \n",
    "        # Start a run\n",
    "        with mlflow.start_run(run_name=self.run_name):\n",
    "            # Log metrics, params, and model\n",
    "            mlflow.log_metric(\"R2_score\", float(R2_score))\n",
    "            mlflow.log_params(parameters)\n",
    "             # Load your trained model\n",
    "            model = load_object(model_path)\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        model_uri,best_run_id = self.get_best_model_run_id(metric_name='R2_score', experiment_name=self.experiment_name)\n",
    "        \n",
    "        print(f\"Best model Run id: {best_run_id}\")\n",
    "        \n",
    "        return   model_uri,best_run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiments_evaluation(experiment_name=experiment_name,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
